import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

from build_dataset import preprocessing_function

model = AutoModelForCausalLM.from_pretrained(
    "results/checkpoint-245/merged_checkpoint",
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    device_map="auto",
)

tokenizer = AutoTokenizer.from_pretrained(
    "results/checkpoint-245/merged_checkpoint",
    trust_remote_code=True,
    use_fast=False,
)

messages = [
    {
        "role": "system",
        "content": "You are é˜¿è•Šå¨… and you are chatting in a QQ group. ",
        "name": None,
    },
    {"role": "user", "content": "å”å”ï¼š", "name": "å†·æœˆ"},
    {"role": "user", "content": "ä¸ºä»€ä¹ˆæˆ‘æ²¡æœ‰è¿™ä¸ª", "name": "(à¹‘â•¹Ú¡â•¹)â•­ æˆ‘è¦åƒ"},
    {"role": "user", "content": "å¥½å®¶ä¼™ï¼Œéƒ½æè¿™éª—æˆ‘é±¼å®ä¸æ‡‚è‹±æ–‡æ˜¯å§ï¼Ÿ", "name": "boxyit"},
    {"role": "user", "content": "ä¸ºä»€ä¹ˆæˆ‘æ²¡æœ‰è¿™ä¸ª", "name": "-æ©™ç ¸Î¦Ï‰Î¦ğŸ˜¸-"},
    {"role": "user", "content": "ä¸ºä»€ä¹ˆæˆ‘æ²¡æœ‰è¿™ä¸ª", "name": "-æ©™ç ¸Î¦Ï‰Î¦ğŸ˜¸-"},
    {"role": "user", "content": "ä¸ºä»€ä¹ˆæˆ‘æ²¡æœ‰è¿™ä¸ª", "name": "-æ©™ç ¸Î¦Ï‰Î¦ğŸ˜¸-"},
    {"role": "user", "content": "æ€¥æ­»æˆ‘äº†æ€¥æ­»æˆ‘äº†æ€¥æ­»æˆ‘äº†æ€¥æ­»æˆ‘äº†æ€¥æ­»æˆ‘äº†æ€¥æ­»æˆ‘äº†æ€¥æ­»æˆ‘äº†æ€¥æ­»æˆ‘äº†", "name": "-æ©™ç ¸Î¦Ï‰Î¦ğŸ˜¸-"},
    {"role": "user", "content": "ç¨€æœ‰ä»»åŠ¡ï¼Ÿ", "name": "é±¼æœˆ"},
    {"role": "user", "content": "æˆ‘ä¹Ÿæ²¡æœ‰è¿™ä¸ªä»»åŠ¡", "name": "boxyit"},
    {"role": "user", "content": "ä½ ä¸€å¤©æ˜¯æˆ‘çš„äºŒåä¸€å€", "name": "-æ©™ç ¸Î¦Ï‰Î¦ğŸ˜¸-"},
    {"role": "user", "content": "æ€¥æ­»äº†æ€¥æ­»äº†æ€¥æ­»äº†æ€¥æ­»äº†æ€¥æ­»äº†æ€¥æ­»äº†æ€¥æ­»äº†æ€¥æ­»äº†æ€¥æ­»äº†", "name": "-æ©™ç ¸Î¦Ï‰Î¦ğŸ˜¸-"},
    {"role": "user", "content": "æ€ä¹ˆå¸¦ç”µéŸ³äº†", "name": "é›¨åŸ"},
    {"role": "user", "content": "æŒ‚å‘æˆ‘å¥½å—ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­", "name": "-æ©™ç ¸Î¦Ï‰Î¦ğŸ˜¸-"},
    {"role": "user", "content": "æˆ‘ä¹Ÿæƒ³å¼€æŒ‚", "name": "-æ©™ç ¸Î¦Ï‰Î¦ğŸ˜¸-"},
    {"role": "user", "content": "ä¸Šä¸‹ä¸Šä¸‹å·¦å³å·¦å³", "name": "uomi"},
    {"role": "user", "content": "abababab", "name": "å®‰é™çš„åå…”å­"},
    {"role": "user", "content": "æŒ‚å‘æˆ‘å¥½å—ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­", "name": "å­¤ç‹¬æ—¶ä»£çš„é¥è¡Œ"},
    {"role": "user", "content": "åŠ©æ•™ä¸è®©å¸¦è€³æœºï¼ŒçœŸä¸¶çœ‹ç›´æ’­", "name": "å­¤ç‹¬æ—¶ä»£çš„é¥è¡Œ"},
    {"role": "assistant", "content": " @å­¤ç‹¬æ—¶ä»£çš„é¥è¡Œ  è®©ä»–ä¸‹æ¥ åŠ©æ•™è®©ä½ æ¥å½“", "name": "é˜¿è•Šå¨…"},
    {"role": "user", "content": "6", "name": "å°‘ã—ã§ã‚‚"},
    {"role": "user", "content": "ä½ è¯´ä½ åœ¨çœ‹ç½‘è¯¾", "name": "å°‘ã—ã§ã‚‚"},
    {"role": "user", "content": "ç­‰æˆ‘è¢«èµ¶å‡ºæ•™å®¤å°±èƒ½çœ‹ç›´æ’­äº† ", "name": "å­¤ç‹¬æ—¶ä»£çš„é¥è¡Œ"},
    {"role": "user", "content": "é˜¿è•Šå¨…çœ‹è¿™ä¸ªè¡¨æƒ…ä¼šä¸ä¼šæƒ³æ‰“å“ˆæ¬ ", "name": "(à¹‘â•¹Ú¡â•¹)â•­ æˆ‘è¦åƒ"},
    {"role": "user", "content": "å“ˆï½æ¬ ï½", "name": "å±±å—æ°´åŒ—çš„å®ˆæœ›"},
    {"role": "user", "content": "å“ˆï½æ¬ ï½", "name": "boxyit"},
    {"role": "user", "content": "å“ˆï½æ¬ ï½", "name": "æœªé—»èŠ±å"},
    {"role": "user", "content": "å“ˆï½æ¬ ï½", "name": "æ˜Ÿç±³Staice"},
    {"role": "user", "content": "å“ˆï½æ¬ ï½", "name": "å­¤ç‹¬æ—¶ä»£çš„é¥è¡Œ"},
    {"role": "user", "content": "å“ˆï½æ¬ ï½", "name": "ç„±ç™½"},
    {"role": "user", "content": "å“ˆï½æ¬ ï½", "name": "kuujaku"},
    {"role": "user", "content": "å“ˆï½æ¬ ï½", "name": "XuAsahi"},
    # {"role": "assistant", "content": "* * *", "name": "é˜¿è•Šå¨…"},
]

data = preprocessing_function(
    {
        "text": {
            "messages": messages,
        }
    },
    tokenizer,
    1024,
    pad_to_max_length=False,
)

input_ids = torch.tensor(data["input_ids"] + [196]).unsqueeze(0).to(model.device)
attention_mask = (
    torch.tensor(data["attention_mask"] + [1]).unsqueeze(0).to(model.device)
)

input_ids = input_ids[:, :512]
output = model.generate(
    input_ids=input_ids,
    attention_mask=attention_mask,
    max_length=1024,
    do_sample=True,
    top_p=0.95,
    top_k=50,
    temperature=0.9,
    num_return_sequences=1,
)

print(tokenizer.decode(output[0]))
